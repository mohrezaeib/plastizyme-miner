{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def split_dataframe_list_to_rows(df, target_column, separator):\n",
    "    \"\"\"Splits column that contains list into row per element of the list.\n",
    "\n",
    "    Args:\n",
    "      df: dataframe to split\n",
    "      target_column: the column containing the values to split\n",
    "      separator: the symbol used to perform the split\n",
    "\n",
    "    Returns:\n",
    "      dataframe with each entry for the target column separated,\n",
    "      with each element moved into a new row.  The values in the\n",
    "      other columns are duplicated across the newly divided rows.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def split_list_to_rows(row, row_accumulator, target_column, separator):\n",
    "        \"\"\"\n",
    "\n",
    "        Args:\n",
    "          row: \n",
    "          row_accumulator: \n",
    "          target_column: \n",
    "          separator: \n",
    "\n",
    "        Returns:\n",
    "\n",
    "        \"\"\"\n",
    "        split_row = row[target_column].split(separator)\n",
    "        for s in split_row:\n",
    "            new_row = row.to_dict()\n",
    "            new_row[target_column] = s\n",
    "            row_accumulator.append(new_row)\n",
    "\n",
    "    new_rows = []\n",
    "    df.apply(split_list_to_rows, axis=1, args=(new_rows, target_column, separator))\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    return new_df\n",
    "\n",
    "\n",
    "def get_id_character_mapping(data, columns):\n",
    "    \"\"\"Creating a mapping between characters and ids given dataframe.\n",
    "\n",
    "    Args:\n",
    "      data: dataframe that contains characters that need to be converted to ids\n",
    "      column: a column of the dataframe that contains characters that need to be converted to ids\n",
    "      columns: \n",
    "\n",
    "    Returns:\n",
    "      id_to_character: dictionary of ids and characters\n",
    "      character_to_id: dictionary of characters and ids\n",
    "\n",
    "    \"\"\"\n",
    "    characters = set([])\n",
    "    for column in columns:\n",
    "        [characters.update(set(val)) for index, val in data[column].iteritems()]\n",
    "    characters = list(sorted(characters))\n",
    "\n",
    "    id_to_character = {i: characters[i] for i in range(len(characters))}\n",
    "    character_to_id = {characters[i]: i for i in range(len(characters))}\n",
    "    return id_to_character, character_to_id\n",
    "\n",
    "\n",
    "def get_category_to_id_mapping(data, column):\n",
    "    \"\"\"Creates two mappings for id and categorical value and vice verse for given column.\n",
    "    Id is a unique identifier of categorical value. Starting from 0.\n",
    "\n",
    "    Args:\n",
    "      data: dataframe that contains categorical values\n",
    "      column: a column of dataframe that contains categorical values for which a mapping from categorical value\n",
    "    to id is needed\n",
    "\n",
    "    Returns:\n",
    "      id_to_category: dictionary of ids and categories\n",
    "      category_to_id: dictionary of categories and ids\n",
    "\n",
    "    \"\"\"\n",
    "    categories = sorted(data[column].unique())\n",
    "    print(\"There are {} unique categories\".format(len(categories)))\n",
    "    id_to_category = {i: categories[i] for i in range(len(categories))}\n",
    "    category_to_id = {categories[i]: i for i in range(len(categories))}\n",
    "    return id_to_category, category_to_id\n",
    "\n",
    "\n",
    "\n",
    "def to_int_feature(data):\n",
    "    \"\"\"\n",
    "    Converts int list to tf Feature\n",
    "    Args:\n",
    "        data: int list to be stored in tf record\n",
    "\n",
    "    Returns:\n",
    "        tf Feature that is used in building tfrecord\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=data))\n",
    "\n",
    "\n",
    "def to_float_feature(data):\n",
    "    \"\"\"\n",
    "    Converts float list to tf Feature\n",
    "    Args:\n",
    "        data: float list to be stored in tf record\n",
    "\n",
    "    Returns:\n",
    "        tf Feature that is used in building tfrecord\n",
    "    \"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=data))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def save_as_npy(path, original_data, columns=[\"Label\", \"sequence\"], ):\n",
    "    \"\"\"Processes a dataframe and stores data into npy file\n",
    "\n",
    "    Args:\n",
    "      filename: the absolute path of the npy file where data should be stored\n",
    "      data: dataframe containing data to be stored\n",
    "      columns: list of columns that should be stored\n",
    "      extension: file extension\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    try:\n",
    "        filename = os.path.join(path, \"data.npy\")\n",
    "        np.save(filename, original_data[columns].values)\n",
    "\n",
    "        print(\"Data was stored in {}\".format(filename))\n",
    "    except Exception as e:\n",
    "        print(\"Something went wrong went writting in to npy file ({})\".format(filename))\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "from common.bio.sequence import Sequence\n",
    "from Bio.SeqIO.FastaIO import SimpleFastaParser\n",
    "from common.bio.constants import ID_TO_AMINO_ACID, AMINO_ACID_TO_ID, NON_STANDARD_AMINO_ACIDS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fasta_to_numpy(path, length):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        path: of the fasta file\n",
    "        separator: used in title of fasta file entry\n",
    "\n",
    "    Returns: numpy array of sequences\n",
    "\n",
    "    \"\"\"\n",
    "    with open(path) as fasta_file:\n",
    "        sequences = []\n",
    "        for title, sequence in SimpleFastaParser(fasta_file):\n",
    "            sequence = sequence[:length]\n",
    "            to_pad = length - len(sequence)\n",
    "            sequence = sequence.rjust(len(sequence) - (to_pad // 2), '0')\n",
    "            sequence = sequence.ljust(length, '0')\n",
    "            if len(sequence) < length:\n",
    "                print(sequence.rjust(to_pad // 2, '0'))\n",
    "                print(to_pad, to_pad//2, length-len(sequence))\n",
    "            np_seq = np.asarray([AMINO_ACID_TO_ID[a] for a in sequence])\n",
    "            sequences.append(np_seq)\n",
    "        return np.stack(sequences, axis= 0)\n",
    "\n",
    "def from_amino_acid_to_id(data, column):\n",
    "    \"\"\"Converts sequences from amino acid to ids\n",
    "\n",
    "    Args:\n",
    "      data: data that contains amino acid that need to be converted to ids\n",
    "      column: a column of the dataframe that contains amino acid that need to be converted to ids\n",
    "\n",
    "    Returns:\n",
    "      array of ids\n",
    "\n",
    "    \"\"\"\n",
    "    return data[column].apply(lambda x: [AMINO_ACID_TO_ID[c] for c in x])\n",
    "\n",
    "\n",
    "def from_id_from_amino_acid(data, column):\n",
    "    \"\"\"Converts sequences from ids to amino acid characters\n",
    "\n",
    "    Args:\n",
    "      data: data that contains ids that need to be converted to amino acid\n",
    "      column: a column of the dataframe that contains ids that need to be converted to amino acid\n",
    "\n",
    "    Returns:\n",
    "      array of amino acid\n",
    "\n",
    "    \"\"\"\n",
    "    return [[ID_TO_AMINO_ACID[id] for id in val] for index, val in data[column].iteritems()]\n",
    "\n",
    "\n",
    "def filter_non_standard_amino_acids(data, column):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "      data: dataframe containing amino acid sequence\n",
    "      column: a column of dataframe that contains amino acid sequence\n",
    "\n",
    "    Returns:\n",
    "      filtered data drame\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data = data[~data[column].str.contains(\"|\".join(NON_STANDARD_AMINO_ACIDS))]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_protein_sequences(sequences, labels=None, d_scores=None):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "      sequences: Protein sequences\n",
    "      id_to_enzyme_class: a dictionary to get enzyme class from its id\n",
    "      labels: Ids  of Enzyme classes (Default value = None)\n",
    "\n",
    "    Returns:\n",
    "      array of Sequence objects\n",
    "    \"\"\"\n",
    "    seqs = []\n",
    "    for index, seq in enumerate(sequences):\n",
    "        label = None if labels is None else labels[index]\n",
    "        d_score = None if d_scores is None else d_scores[index]\n",
    "        seqs.append(Sequence(index, seq, label=label, d_score=d_score))\n",
    "    return seqs\n",
    "\n",
    "\n",
    "def numpy_seqs_to_fasta(sequences, id_to_enzyme_class, labels=None, d_scores=None, strip_zeros=False):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "      sequences: Protein sequences\n",
    "      id_to_enzyme_class: a dictionary to get enzyme class from its id\n",
    "      labels: Ids  of Enzyme classes (Default value = None)\n",
    "      d_scores: Values of discriminator (Default value = None)\n",
    "      strip_zeros: Flag to determine if special characters needs to be escape. Applicable for text in tersorboard\n",
    "    Returns:\n",
    "      array of strings with sequences and additional information\n",
    "\n",
    "    \"\"\"\n",
    "    seqs = get_protein_sequences(sequences, labels, d_scores)\n",
    "    return sequences_to_fasta(seqs, id_to_enzyme_class, True, strip_zeros)\n",
    "\n",
    "\n",
    "def sequences_to_fasta(sequences, id_to_enzyme_class, escape=True, strip_zeros=False):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "      sequences: a list of Sequences object\n",
    "      id_to_enzyme_class: a dictionary to get enzyme class from its id\n",
    "      labels: Ids  of Enzyme classes (Default value = None)\n",
    "      escape: a flag to determine if special characters needs to be escape. Applicable for text in tersorboard\n",
    "      strip_zeros: a flag that determines whether zeros are removed from sequences\n",
    "    Returns:\n",
    "      string with sequences and additional information that mimics fasta format\n",
    "\n",
    "    \"\"\"\n",
    "    return os.linesep.join([seq.get_seq_in_fasta(id_to_enzyme_class, escape, strip_zeros) for seq in sequences])\n",
    "\n",
    "\n",
    "def print_protein_seq(sequences, id_to_enzyme_class, labels=None, d_scores=None):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "      sequences: Protein sequences\n",
    "      id_to_enzyme_class: a dictionary to get enzyme class from its id\n",
    "      labels: Ids  of Enzyme classes (Default value = None)\n",
    "      d_scores: Values of discriminator (Default value = None)\n",
    "\n",
    "    Returns:\n",
    "      Signal for DONE\n",
    "\n",
    "    \"\"\"\n",
    "    print(\"\\n\".join(numpy_seqs_to_fasta(sequences, id_to_enzyme_class, labels, d_scores)))\n",
    "    return \"DONE\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fasta_to_numpy(path, length):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        path: of the fasta file\n",
    "        separator: used in title of fasta file entry\n",
    "\n",
    "    Returns: numpy array of sequences\n",
    "\n",
    "    \"\"\"\n",
    "    with open(path) as fasta_file:\n",
    "        sequences = []\n",
    "        for title, sequence in SimpleFastaParser(fasta_file):\n",
    "            sequence = sequence[:length]\n",
    "            to_pad = length - len(sequence)\n",
    "            sequence = sequence.rjust(len(sequence) - (to_pad // 2), '0')\n",
    "            sequence = sequence.ljust(length, '0')\n",
    "            if len(sequence) < length:\n",
    "                print(sequence.rjust(to_pad // 2, '0'))\n",
    "                print(to_pad, to_pad//2, length-len(sequence))\n",
    "            np_seq = np.asarray([AMINO_ACID_TO_ID[a] for a in sequence])\n",
    "            sequences.append(np_seq)\n",
    "        return np.stack(sequences, axis= 0)\n",
    "\n",
    "\n",
    "def generate_random_seqs(data, column='sequence', n_seqs=1000):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        data: Dataframe that contains sequences\n",
    "        column: a name of the column which contains sequences\n",
    "\n",
    "    Returns:\n",
    "        Randomly generated sequences based on frequency of each element\n",
    "\n",
    "    \"\"\"\n",
    "    results = Counter(data[column].str.cat())\n",
    "    counts = [i[1] for i in sorted(results.items())]\n",
    "    prop = np.asarray(counts) / sum(list(counts))\n",
    "    lengths = data.sequence.str.len().sample(n_seqs).values + int(np.random.normal(scale=3))\n",
    "    seqs = []\n",
    "    for i in range(n_seqs):\n",
    "        r = np.random.choice(np.arange(1, 21), p=prop, size=lengths[i])\n",
    "        seq = \">R_{}\\nM\".format(i)\n",
    "        for a in r:\n",
    "            seq = seq + ID_TO_AMINO_ACID[a]\n",
    "        seqs.append(seq)\n",
    "    return seqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fasta_to_pandas(path, separator=\";\"):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        path: of the fasta file\n",
    "        separator: used in title of fasta file entry\n",
    "\n",
    "    Returns: pandas dataframe with 3 columns (id, title, sequence)\n",
    "\n",
    "    \"\"\"\n",
    "    with open(path) as fasta_file:\n",
    "        identifiers, sequences, titles, labels = [], [], [],[]\n",
    "        id =0\n",
    "        for title, sequence in SimpleFastaParser(fasta_file):\n",
    "            title_parts = title.split(separator, 1)\n",
    "            identifiers.append(id)  # First word is ID\n",
    "            titles.append(\"1.1.1.37_\" + str(id))\n",
    "            sequences.append(sequence)\n",
    "            labels.append(\"labelx\")\n",
    "            id +=1\n",
    "        return pd.DataFrame({\"id\": identifiers, \"title\": titles,\"Label\":labels, \"sequence\": sequences})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROW_DATA =\"./preprocessing/raw-data.fasta\"\n",
    "FASTA_CLEAN = \"./preprocessing/data/uniprot-clean.fasta\"\n",
    "FASTA_TRAIN=\"./preprocessing/data/train_sequences.fasta\"\n",
    "FASTA_VAL=\"./preprocessing/data/val_sequences.fasta\"\n",
    "TRAIN=\"./preprocessing/data/train/\"\n",
    "VAL=\"./preprocessing/data/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "def df_to_fasta_clean(df , path):\n",
    "\n",
    "    output_handle1 = open(path, \"a\")\n",
    "    df = filter_non_standard_amino_acids(df , \"sequence\")\n",
    "    for index , row in df.iterrows():\n",
    "        record = SeqRecord(\n",
    "        Seq(row['sequence']),\n",
    "        id=str(row['title']),\n",
    "        # id=str(0),\n",
    "\n",
    "        name= '',\n",
    "        description=row['title'],\n",
    "        )\n",
    "        SeqIO.write(record, output_handle1, 'fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = fasta_to_pandas(ROW_DATA)\n",
    "df_to_fasta_clean(df[:-15] , FASTA_TRAIN)\n",
    "df_to_fasta_clean(df[-15:] , FASTA_VAL)\n",
    "df_train = fasta_to_pandas(FASTA_TRAIN)\n",
    "df_val = fasta_to_pandas(FASTA_VAL)\n",
    "\n",
    "\n",
    "df_train[\"sequence\"] = from_amino_acid_to_id(df_train , \"sequence\")\n",
    "df_val[\"sequence\"] = from_amino_acid_to_id(df_val , \"sequence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_tfrecords_multithreaded(path, original_data, columns=[\"sequence\"], group_by_col=\"Label\"):\n",
    "    \"\"\"Provided data gets splitted in to groups and processed concurrently.\n",
    "    The outcome of this is a file per group.\n",
    "\n",
    "    Args:\n",
    "      path: Location where files should be stored\n",
    "      original_data: dataframe which should be converted into files\n",
    "      columns: a  list of columns which should be stored as sequences (Default value = [\"sequence\"])\n",
    "      group_by_col: a column name by which split data into groups (Default value = \"Label\")\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    threading_start = time.time()\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = []\n",
    "    data = original_data.groupby(group_by_col)\n",
    "    for group_id in data.groups:\n",
    "        if isinstance(group_id, str):\n",
    "            group_name = group_id.replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "        elif isinstance(group_id, int):\n",
    "            group_name = str(group_id)\n",
    "        else:\n",
    "            group_name = \"_\".join([str(e) for e in group_id])\n",
    "        filename = os.path.join(path, group_name)\n",
    "        args = (filename, data.get_group(group_id), columns)\n",
    "        t = threading.Thread(target=save_as_tfrecords, args=args)\n",
    "        t.start()\n",
    "        threads.append(t)\n",
    "    coord.join(threads)\n",
    "    print(\"Completed all threads in {} seconds\".format(time.time() - threading_start))\n",
    "\n",
    "\n",
    "def save_as_tfrecords(filename, data, columns=[\"sequence\"], extension=\"tfrecords\"):\n",
    "    \"\"\"Processes a dataframe and stores data into tfrecord file\n",
    "\n",
    "    Args:\n",
    "      filename: the absolute path of the tfrecords file where data should be stored\n",
    "      data: dataframe containing data will be converted into tfrecord\n",
    "      columns: list of columns that should be stored as varying-length sequences (Default value = [\"sequence\"])\n",
    "      extension: file extension\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        filename = \"{}.{}\".format(filename, extension)\n",
    "        with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "            for index, row in data.iterrows():\n",
    "                feature = {\n",
    "                    'label': to_int_feature([row[0]])\n",
    "                }\n",
    "                for col_name in columns:\n",
    "                    value = row[col_name]\n",
    "                    if isinstance(value, int):\n",
    "                        feature[col_name] = to_int_feature([value])\n",
    "                    elif isinstance(value, float):\n",
    "                        feature[col_name] = to_float_feature([value])\n",
    "                    elif not isinstance(value, (list,)) and not (isinstance (value, int) or ((value.dtype == np.float32) or (value.dtype == np.float64))):\n",
    "                        feature[col_name] = to_float_feature(value)\n",
    "                    else:\n",
    "                        feature[col_name] = to_int_feature(value)\n",
    "                        feature['length_' + col_name] = to_int_feature([len(value)])\n",
    "                # print(feature)\n",
    "                example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "                writer.write(example.SerializeToString())\n",
    "\n",
    "        print(\"Data was stored in {}\".format(filename))\n",
    "    except Exception as e:\n",
    "        print(\"Something went wrong went writting in to tfrecords file\")\n",
    "        print(\"Error is \", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data was stored in ./preprocessing/data/train/labelx.tfrecords\n",
      "Completed all threads in 1.0169885158538818 seconds\n",
      "Data was stored in ./preprocessing/data/val/labelx.tfrecords\n",
      "Completed all threads in 1.002518653869629 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_as_tfrecords_multithreaded(TRAIN,df_train)\n",
    "save_as_tfrecords_multithreaded(VAL,df_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makeblastdb -in train_sequences.fasta -out db_train  -parse_seqids -blastdb_version 5  -title \"train sequence\" -dbtype prot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shutil.copytree('/home/mohre/B/d/Research/templates/ProteinGAN/src/a','/home/mohre/B/d/Research/templates/ProteinGAN/data/protein/x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.14",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "20b1e5ec344bee7c0aac9d6e6ab6162f5e1d002bed37759a4820a86b7c890098"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
